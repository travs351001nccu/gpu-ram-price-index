"""
Consumer Electronics Price Index - Main Crawler
Simplified all-in-one version for daily execution
Author: Travis Cua
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import re
import json
from pathlib import Path
import psycopg2
from psycopg2.extras import Json
from db_config import config

def load_taxonomy():
    """Load product taxonomy"""
    with open('product_taxonomy.json', 'r', encoding='utf-8') as f:
        return json.load(f)

def fetch_coolpc_data():
    """Fetch data from Coolpc"""
    url = "https://www.coolpc.com.tw/evaluate.php"
    headers = {"User-Agent": "Mozilla/5.0"}
    
    print(" Fetching data from Coolpc...")
    res = requests.get(url, headers=headers, timeout=30)
    res.encoding = 'big5'
    soup = BeautifulSoup(res.text, 'html.parser')
    
    all_products = []
    for select in soup.find_all('select'):
        for group in select.find_all('optgroup'):
            category = group.get('label', '')
            for opt in group.find_all('option'):
                raw_text = opt.text.strip()
                if not raw_text or raw_text.startswith("---"):
                    continue
                
                price_match = re.search(r'\$\s*(\d+)', raw_text)
                if price_match:
                    all_products.append({
                        'ÂéüÂßãÂàÜÈ°û': category,
                        'ÂïÜÂìÅÂêçÁ®±': raw_text.split('$')[0].strip().rstrip(','),
                        'ÂÉπÊ†º': int(price_match.group(1)),
                        'ÂÆåÊï¥Ë≥áË®ä': raw_text
                    })
    
    print(f" Fetched {len(all_products):,} products")
    return pd.DataFrame(all_products)

def classify_products(df, taxonomy):
    """Classify products using taxonomy"""
    print("  Classifying products...")
    
    classified = []
    global_exclusions = [kw.lower() for kw in taxonomy.get('global_exclusions', {}).get('keywords', [])]
    
    for _, row in df.iterrows():
        product_lower = row['ÂïÜÂìÅÂêçÁ®±'].lower()
        
        # Global exclusions
        if any(ex in product_lower for ex in global_exclusions):
            continue
        
        # Match categories
        for cat_name, cat_config in taxonomy['categories'].items():
            if not any(kw in row['ÂéüÂßãÂàÜÈ°û'].lower() for kw in cat_config.get('category_keywords', [])):
                continue
            
            if any(ex.lower() in product_lower for ex in cat_config.get('exclude_keywords', [])):
                continue
            
            # Match generations
            for gen_name, gen_config in cat_config.get('generations', {}).items():
                matched = False
                
                if 'models' in gen_config:
                    if any(model.lower() in product_lower for model in gen_config['models']):
                        matched = True
                
                if 'capacities' in gen_config:
                    for capacity, keywords in gen_config['capacities'].items():
                        if any(kw.lower() in product_lower for kw in keywords):
                            matched = True
                            break
                
                if matched:
                    price_range = gen_config.get('price_range', [0, 999999])
                    if price_range[0] <= row['ÂÉπÊ†º'] <= price_range[1]:
                        classified.append({
                            'È°ûÂà•': cat_name,
                            '‰∏ñ‰ª£': gen_name,
                            'ÂïÜÂìÅÂêçÁ®±': row['ÂïÜÂìÅÂêçÁ®±'],
                            'ÂÉπÊ†º': row['ÂÉπÊ†º'],
                            'ÂéüÂßãÂàÜÈ°û': row['ÂéüÂßãÂàÜÈ°û'],
                            'ÂÆåÊï¥Ë≥áË®ä': row['ÂÆåÊï¥Ë≥áË®ä']
                        })
                    break
            break
    
    print(f" Classified {len(classified):,} products")
    return pd.DataFrame(classified)

def save_to_database(df):
    """Save to PostgreSQL"""
    try:
        conn = psycopg2.connect(**config.get_psycopg2_params())
        cursor = conn.cursor()
        today = datetime.now().date()
        new_products = 0
        
        print(" Saving to database...")
        
        for _, row in df.iterrows():
            brand = row['ÂïÜÂìÅÂêçÁ®±'].split()[0] if row['ÂïÜÂìÅÂêçÁ®±'] else None
            
            # Insert/update product
            cursor.execute("""
                INSERT INTO products (category, generation, product_name, brand, first_seen, last_seen, source)
                VALUES (%s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (product_name, generation) 
                DO UPDATE SET last_seen = EXCLUDED.last_seen, is_active = TRUE, source = EXCLUDED.source
                RETURNING product_id
            """, (row['È°ûÂà•'], row['‰∏ñ‰ª£'], row['ÂïÜÂìÅÂêçÁ®±'], brand, today, today, 'Coolpc'))
            
            product_id = cursor.fetchone()[0]
            
            # Insert price
            cursor.execute("""
                INSERT INTO daily_prices (date, product_id, price, source, raw_info)
                VALUES (%s, %s, %s, %s, %s)
                ON CONFLICT (date, product_id) DO UPDATE SET price = EXCLUDED.price, source = EXCLUDED.source
            """, (today, product_id, row['ÂÉπÊ†º'], 'Coolpc', Json({'ÂÆåÊï¥Ë≥áË®ä': row['ÂÆåÊï¥Ë≥áË®ä']})))
            
            new_products += 1
        
        # Generate index
        cursor.execute("""
            INSERT INTO daily_index (date, category, generation, avg_price, min_price, max_price, median_price, std_price, product_count, volatility)
            SELECT 
                %s, category, generation,
                AVG(price)::numeric(10,2), MIN(price), MAX(price), 
                PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY price)::numeric(10,2),
                STDDEV(price)::numeric(10,2), COUNT(*),
                (STDDEV(price) / AVG(price) * 100)::numeric(5,2)
            FROM (
                SELECT p.category, p.generation, dp.price
                FROM products p
                JOIN daily_prices dp ON p.product_id = dp.product_id
                WHERE dp.date = %s
            ) sub
            GROUP BY category, generation
            ON CONFLICT (date, category, generation) DO UPDATE 
            SET avg_price = EXCLUDED.avg_price, min_price = EXCLUDED.min_price
        """, (today, today))
        
        conn.commit()
        cursor.close()
        conn.close()
        
        print(f" Saved {new_products} products to database")
        return True
        
    except Exception as e:
        print(f" Database error: {e}")
        return False

def main():
    """Main execution"""
    print("=" * 60)
    print(" Consumer Electronics Price Index Crawler")
    print("=" * 60)
    
    # Load taxonomy
    taxonomy = load_taxonomy()
    
    # ===== COOLPC =====
    print("\nüì¶ Source 1: Coolpc")
    raw_df = fetch_coolpc_data()
    if not raw_df.empty:
        classified_df = classify_products(raw_df, taxonomy)
        if not classified_df.empty:
            save_to_database(classified_df)
    
    # ===== PCHOME =====
    print("\nüì¶ Source 2: PChome 24h")
    try:
        from pchome_crawler import fetch_pchome_data, save_pchome_to_database
        pchome_df = fetch_pchome_data()
        if not pchome_df.empty:
            save_pchome_to_database(pchome_df)
    except ImportError as e:
        print(f"  Warning: PChome crawler not available: {e}")
    except Exception as e:
        print(f"  Error fetching PChome data: {e}")
    
    # Display combined summary
    print("\n" + "=" * 60)
    print(" Today's Market Index (Combined)")
    print("=" * 60)
    
    try:
        conn = psycopg2.connect(**config.get_psycopg2_params())
        summary_query = """
            SELECT 
                COALESCE(p.source, 'Coolpc') as source,
                p.category,
                p.generation,
                COUNT(*) as count,
                ROUND(AVG(dp.price)::numeric, 0) as avg_price
            FROM products p
            JOIN daily_prices dp ON p.product_id = dp.product_id
            WHERE dp.date = CURRENT_DATE
            GROUP BY p.source, p.category, p.generation
            ORDER BY p.category, p.generation, p.source
        """
        summary_df = pd.read_sql_query(summary_query, conn)
        conn.close()
        
        if not summary_df.empty:
            print(summary_df.to_string(index=False))
    except Exception as e:
        print(f"  Could not load summary: {e}")
    
    print("\n‚úÖ Crawler completed successfully!")
    print("=" * 60)

if __name__ == "__main__":
    main()

